import datetime
import logging
import time
from typing import Dict, Any, Optional, Union, List

logger = logging.getLogger(__name__)

def format_number(value: Union[int, float, str]) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
    if isinstance(value, (int, float)):
        if value >= 1000000000:
            return f"${value / 1000000000:.2f}B"
        elif value >= 1000000:
            return f"${value / 1000000:.2f}M"
        elif value >= 1000:
            return f"${value / 1000:.2f}K"
        else:
            return f"${value:.2f}"
    elif isinstance(value, str):
        try:
            value = float(value)
            return format_number(value)
        except (ValueError, TypeError):
            return value
    else:
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"

def calculate_token_age(timestamp: Optional[int]) -> str:
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤–æ–∑—Ä–∞—Å—Ç —Ç–æ–∫–µ–Ω–∞ –æ—Ç –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π —Ä–∞–∑–±–∏–≤–∫–æ–π."""
    if not timestamp:
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
    
    try:
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ timestamp –∏–∑ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥ –≤ —Å–µ–∫—É–Ω–¥—ã
        creation_time = datetime.datetime.fromtimestamp(timestamp / 1000)
        now = datetime.datetime.now()
        delta = now - creation_time
        
        days = delta.days
        hours = delta.seconds // 3600
        minutes = (delta.seconds % 3600) // 60
        
        result = []
        
        if days > 0:
            days_str = "–¥–µ–Ω—å" if days == 1 else "–¥–Ω—è" if 1 < days < 5 else "–¥–Ω–µ–π"
            result.append(f"{days} {days_str}")
        
        if hours > 0:
            hours_str = "—á–∞—Å" if hours == 1 else "—á–∞—Å–∞" if 1 < hours < 5 else "—á–∞—Å–æ–≤"
            result.append(f"{hours} {hours_str}")
        
        if minutes > 0 and days == 0:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –º–∏–Ω—É—Ç—ã —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø—Ä–æ—à–ª–æ –º–µ–Ω—å—à–µ –¥–Ω—è
            minutes_str = "–º–∏–Ω—É—Ç–∞" if minutes == 1 else "–º–∏–Ω—É—Ç—ã" if 1 < minutes < 5 else "–º–∏–Ω—É—Ç"
            result.append(f"{minutes} {minutes_str}")
        
        if not result:
            return "–ú–µ–Ω–µ–µ –º–∏–Ω—É—Ç—ã"
        
        return " ".join(result)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ –≤–æ–∑—Ä–∞—Å—Ç–∞ —Ç–æ–∫–µ–Ω–∞: {e}")
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"

def time_elapsed_since(timestamp: float) -> str:
    """–í—ã—á–∏—Å–ª—è–µ—Ç –ø—Ä–æ—à–µ–¥—à–µ–µ –≤—Ä–µ–º—è —Å —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –º–æ–º–µ–Ω—Ç–∞."""
    if not timestamp:
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
    
    try:
        now = time.time()
        delta_seconds = now - timestamp
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ timedelta –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ —Ä–∞—Å—á–µ—Ç–∞
        delta = datetime.timedelta(seconds=delta_seconds)
        
        days = delta.days
        hours = delta.seconds // 3600
        minutes = (delta.seconds % 3600) // 60
        
        result = []
        
        if days > 0:
            days_str = "–¥–µ–Ω—å" if days == 1 else "–¥–Ω—è" if 1 < days < 5 else "–¥–Ω–µ–π"
            result.append(f"{days} {days_str}")
        
        if hours > 0:
            hours_str = "—á–∞—Å" if hours == 1 else "—á–∞—Å–∞" if 1 < hours < 5 else "—á–∞—Å–æ–≤"
            result.append(f"{hours} {hours_str}")
        
        if minutes > 0 and days == 0:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –º–∏–Ω—É—Ç—ã —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø—Ä–æ—à–ª–æ –º–µ–Ω—å—à–µ –¥–Ω—è
            minutes_str = "–º–∏–Ω—É—Ç–∞" if minutes == 1 else "–º–∏–Ω—É—Ç—ã" if 1 < minutes < 5 else "–º–∏–Ω—É—Ç"
            result.append(f"{minutes} {minutes_str}")
        
        if not result:
            return "–ú–µ–Ω–µ–µ –º–∏–Ω—É—Ç—ã"
        
        return " ".join(result)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ –ø—Ä–æ—à–µ–¥—à–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏: {e}")
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"

def process_token_data(token_data: Dict[str, Any]) -> Dict[str, Any]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–∫–µ–Ω–µ."""
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω—É–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    base_token = token_data.get('baseToken', {})
    ticker = base_token.get('symbol', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ').upper()
    ticker_address = base_token.get('address', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
    pair_address = token_data.get('pairAddress', '')
    chain_id = token_data.get('chainId', '')
    
    # –ü–æ–ª—É—á–∞–µ–º market cap, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
    market_cap = token_data.get('fdv')
    raw_market_cap = market_cap  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
    market_cap_formatted = format_number(market_cap)
    
    # –°–æ–∑–¥–∞–µ–º —Å—Å—ã–ª–∫–∏
    dexscreener_link = f"https://dexscreener.com/{chain_id}/{pair_address}"
    axiom_link = f"https://axiom.trade/meme/{pair_address}"
    
    # –ü–æ–ª—É—á–∞–µ–º –æ–±—ä–µ–º —Ç–æ—Ä–≥–æ–≤
    volume_data = token_data.get('volume', {})
    volume_5m = volume_data.get('m5')
    volume_1h = volume_data.get('h1')
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ–±—ä–µ–º—ã
    volume_5m_formatted = format_number(volume_5m)
    volume_1h_formatted = format_number(volume_1h)
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–∞
    pair_created_at = token_data.get('pairCreatedAt')
    token_age = calculate_token_age(pair_created_at)
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç—è—Ö –∏ —Å–∞–π—Ç–∞—Ö
    info = token_data.get('info', {})
    websites = info.get('websites', [])
    socials = info.get('socials', [])
    
    return {
        'ticker': ticker,
        'ticker_address': ticker_address,
        'pair_address': pair_address,
        'chain_id': chain_id,
        'market_cap': market_cap_formatted,
        'raw_market_cap': raw_market_cap,
        'volume_5m': volume_5m_formatted,
        'volume_1h': volume_1h_formatted,
        'token_age': token_age,
        'dexscreener_link': dexscreener_link,
        'axiom_link': axiom_link,
        'websites': websites,
        'socials': socials
    }

def format_message(token_info: Dict[str, Any], initial_data: Optional[Dict[str, Any]] = None) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ç–æ–∫–µ–Ω–µ."""
    message = f"ü™ô *–¢–∏–∫–µ—Ä*: {token_info['ticker']}\n"
    message += f"üìù *–ê–¥—Ä–µ—Å*: `{token_info['ticker_address']}`\n\n"
    
    message += f"üí∞ *Market Cap*: {token_info['market_cap']}\n"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ –º–∞—Ä–∫–µ—Ç –∫–∞–ø–∞, –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–∞—á–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    if (initial_data and 'raw_market_cap' in initial_data and 
        initial_data['raw_market_cap'] and token_info['raw_market_cap']):
        initial_mcap = initial_data['raw_market_cap']
        current_mcap = token_info['raw_market_cap']
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–Ω–æ–∂–∏—Ç–µ–ª—å
        if initial_mcap > 0:
            multiplier = current_mcap / initial_mcap
            
            # –ï—Å–ª–∏ –º–Ω–æ–∂–∏—Ç–µ–ª—å –±–æ–ª—å—à–µ 2, –¥–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            if multiplier >= 2:
                # –û–∫—Ä—É–≥–ª—è–µ–º –º–Ω–æ–∂–∏—Ç–µ–ª—å –¥–æ —Ü–µ–ª–æ–≥–æ —á–∏—Å–ª–∞, –µ—Å–ª–∏ –æ–Ω –±–æ–ª—å—à–µ 10, –∏–Ω–∞—á–µ –¥–æ –æ–¥–Ω–æ–≥–æ –∑–Ω–∞–∫–∞ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π
                mult_formatted = int(multiplier) if multiplier >= 10 else round(multiplier, 1)
                message += f"üî• *–†–æ—Å—Ç: x{mult_formatted}* –æ—Ç –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è!\n"
    
    message += f"‚è± *–í–æ–∑—Ä–∞—Å—Ç —Ç–æ–∫–µ–Ω–∞*: {token_info['token_age']}\n\n"
    
    # –ë–ª–æ–∫ —Å –æ–±—ä–µ–º–∞–º–∏ —Ç–æ—Ä–≥–æ–≤
    volumes_block = ""
    if token_info['volume_5m'] != "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ":
        volumes_block += f"üìà *–û–±—ä–µ–º (5–º)*: {token_info['volume_5m']}\n"
        
    if token_info['volume_1h'] != "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ":
        volumes_block += f"üìà *–û–±—ä–µ–º (1—á)*: {token_info['volume_1h']}\n"
    
    if volumes_block:
        message += volumes_block + "\n"
    
    # –ë–ª–æ–∫ —Å —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ —Ç–æ—Ä–≥–æ–≤—ã–µ –ø–ª–æ—â–∞–¥–∫–∏
    message += f"üîé *–°—Å—ã–ª–∫–∏*: [DexScreener]({token_info['dexscreener_link']}) | [Axiom Trade]({token_info['axiom_link']})\n\n"
    
    # –ë–ª–æ–∫ —Å —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ —Å–∞–π—Ç—ã
    if token_info['websites']:
        website_links = [f"[{website.get('label', 'Website')}]({website.get('url', '')})" 
                         for website in token_info['websites'] if website.get('url')]
        
        if website_links:
            message += f"üåê *–°–∞–π—Ç—ã*: {' | '.join(website_links)}\n"
    
    # –ë–ª–æ–∫ —Å —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ —Å–æ—Ü—Å–µ—Ç–∏
    if token_info['socials']:
        social_links = [f"[{social.get('type', '').capitalize()}]({social.get('url', '')})" 
                        for social in token_info['socials'] if social.get('url') and social.get('type')]
        
        if social_links:
            message += f"üì± *–°–æ—Ü—Å–µ—Ç–∏*: {' | '.join(social_links)}\n"
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫—É –≤—Ä–µ–º–µ–Ω–∏ –∏ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    current_time = datetime.datetime.now().strftime("%H:%M:%S")
    message += f"\n_–û–±–Ω–æ–≤–ª–µ–Ω–æ: {current_time}_"
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ, –µ—Å–ª–∏ –æ–Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–∞
    if initial_data:
        message += f"\n_–ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å: {initial_data['time']}_"
        message += f"\n_–ù–∞—á–∞–ª—å–Ω—ã–π Market Cap: {initial_data['market_cap']}_"
    
    return message

def remove_specific_token(token_storage, token_query: str):
    """
    –£–¥–∞–ª—è–µ—Ç —É–∫–∞–∑–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω –∏–∑ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞.
    
    Args:
        token_storage: –ú–æ–¥—É–ª—å —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤
        token_query: –ó–∞–ø—Ä–æ—Å –∏–ª–∏ —Ç–∏–∫–µ—Ä —Ç–æ–∫–µ–Ω–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
    
    Returns:
        dict: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± —É–¥–∞–ª–µ–Ω–Ω–æ–º —Ç–æ–∫–µ–Ω–µ –∏–ª–∏ None, –µ—Å–ª–∏ —Ç–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω
    """
    # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º –≤—Å–µ —Ç–æ–∫–µ–Ω—ã
    all_tokens = token_storage.get_all_tokens()
    
    # –ò—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –∑–∞–ø—Ä–æ—Å—É/–∞–¥—Ä–µ—Å—É
    if token_query in all_tokens:
        token_data = all_tokens[token_query]
        token_storage.remove_token_data(token_query)
        return {"query": token_query, "data": token_data}
    
    # –ï—Å–ª–∏ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–µ—Ç, –∏—â–µ–º –ø–æ —Ç–∏–∫–µ—Ä—É (–Ω–µ—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∫ —Ä–µ–≥–∏—Å—Ç—Ä—É)
    for query, data in all_tokens.items():
        ticker = ""
        if 'token_info' in data and 'ticker' in data['token_info']:
            ticker = data['token_info']['ticker']
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–∏–∫–µ—Ä–∞ (–Ω–µ—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∫ —Ä–µ–≥–∏—Å—Ç—Ä—É)
        if ticker.lower() == token_query.lower():
            token_data = all_tokens[query]
            token_storage.remove_token_data(query)
            return {"query": query, "data": token_data}
    
    # –ï—Å–ª–∏ —Ç–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω
    return None

def format_tokens_list(tokens_data: Dict[str, Dict[str, Any]], page: int = 0, tokens_per_page: int = 10) -> tuple:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø—Ä–æ—Ü–µ–Ω—Ç–∞–º–∏ –æ—Ç ATH.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ç–µ–∂ (message, total_pages, current_page)
    """
    if not tokens_data:
        return ("–ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–ø–∏—Å–∫–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã—Ö.", 1, 0)
    
    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∫—Ä—ã—Ç—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    hidden_info = ""
    try:
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥—É–ª—å token_storage, –µ—Å–ª–∏ –æ–Ω –µ—â–µ –Ω–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω
        import token_storage as ts
        hidden_tokens_count = len(ts.get_hidden_tokens())
        hidden_info = f" (—Å–∫—Ä—ã—Ç—ã—Ö: {hidden_tokens_count})" if hidden_tokens_count > 0 else ""
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–∫—Ä—ã—Ç—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: {str(e)}")
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
    token_info_list = []
    
    try:
        for query, data in tokens_data.items():
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∫—Ä—ã—Ç—ã–µ —Ç–æ–∫–µ–Ω—ã
            if data.get('hidden', False):
                continue
                
            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏ –Ω–∞ None
            token_info = {}
            token_info['query'] = query
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–∏–∫–µ—Ä
            token_info['ticker'] = query
            if data.get('token_info', {}).get('ticker'):
                token_info['ticker'] = data['token_info']['ticker']
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Ä–µ–º—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –µ–≥–æ –≤ –ø–æ–ª–Ω—É—é –¥–∞—Ç—É –∏ –≤—Ä–µ–º—è
            token_info['initial_time'] = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
            token_info['added_date'] = ""
            
            if data.get('added_time'):
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º timestamp –≤ –¥–∞—Ç—É –∏ –≤—Ä–µ–º—è
                import datetime
                added_datetime = datetime.datetime.fromtimestamp(data.get('added_time', 0))
                token_info['initial_time'] = added_datetime.strftime("%H:%M:%S")
                token_info['added_date'] = added_datetime.strftime("%Y-%m-%d")
                token_info['full_datetime'] = added_datetime.strftime("%Y-%m-%d %H:%M:%S")
            elif data.get('initial_data', {}).get('time'):
                token_info['initial_time'] = data['initial_data']['time']
            
            # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π –º–∞—Ä–∫–µ—Ç –∫–∞–ø
            token_info['initial_market_cap'] = 0
            if data.get('initial_data', {}).get('raw_market_cap'):
                token_info['initial_market_cap'] = data['initial_data']['raw_market_cap']
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π –º–∞—Ä–∫–µ—Ç –∫–∞–ø
            token_info['current_market_cap'] = 0
            if data.get('token_info', {}).get('raw_market_cap'):
                token_info['current_market_cap'] = data['token_info']['raw_market_cap']
            
            # –ü–æ–ª—É—á–∞–µ–º ATH –º–∞—Ä–∫–µ—Ç –∫–∞–ø
            token_info['ath_market_cap'] = data.get('ath_market_cap', 0)
            
            # –ï—Å–ª–∏ ATH –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–ª–∏ –º–µ–Ω—å—à–µ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π –∫–∞–∫ ATH
            if not token_info['ath_market_cap'] or (token_info['initial_market_cap'] > token_info['ath_market_cap']):
                token_info['ath_market_cap'] = token_info['initial_market_cap']
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –≤—ã—á–∏—Å–ª—è–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç—ã –¥–ª—è ATH –∏ —Ç–µ–∫—É—â–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
            token_info['ath_percent'] = 0
            if token_info['initial_market_cap'] and token_info['ath_market_cap'] and token_info['initial_market_cap'] > 0:
                token_info['ath_percent'] = ((token_info['ath_market_cap'] / token_info['initial_market_cap']) - 1) * 100
            
            token_info['curr_percent'] = 0
            if token_info['initial_market_cap'] and token_info['current_market_cap'] and token_info['initial_market_cap'] > 0:
                token_info['curr_percent'] = ((token_info['current_market_cap'] / token_info['initial_market_cap']) - 1) * 100
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ DexScreener
            token_info['dexscreener_link'] = "#"
            if data.get('token_info', {}).get('dexscreener_link'):
                token_info['dexscreener_link'] = data['token_info']['dexscreener_link']
            
            token_info_list.append(token_info)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã –ø–æ –ø—Ä–æ—Ü–µ–Ω—Ç—É —Ä–æ—Å—Ç–∞ ATH (–æ—Ç –Ω–∞–∏–±–æ–ª—å—à–µ–≥–æ –∫ –Ω–∞–∏–º–µ–Ω—å—à–µ–º—É)
        token_info_list.sort(key=lambda x: x.get('ath_percent', 0), reverse=True)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return ("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.", 1, 0)
    
    # –†–∞—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü
    total_tokens = len(token_info_list)
    total_pages = (total_tokens + tokens_per_page - 1) // tokens_per_page  # –û–∫—Ä—É–≥–ª–µ–Ω–∏–µ –≤–≤–µ—Ä—Ö
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    if page < 0:
        page = 0
    elif page >= total_pages and total_pages > 0:
        page = total_pages - 1
    
    # –ù–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü –¥–∏–∞–ø–∞–∑–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    start_idx = page * tokens_per_page
    end_idx = min(start_idx + tokens_per_page, total_tokens)
    
    # –¢–æ–∫–µ–Ω—ã –¥–ª—è —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    page_tokens = token_info_list[start_idx:end_idx]
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏—è
    message = f"üìã *–°–ø–∏—Å–æ–∫ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ ({total_tokens} —à—Ç.){hidden_info}*\n"
    message += f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page + 1} –∏–∑ {total_pages}\n\n"
    
    # –ó–∞–≥—Ä—É–∑–∏–º tracker_db –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–æ–¥–∑–∏ —Ç–æ–∫–µ–Ω–æ–≤
    tracker_emojis = {}
    try:
        import json
        import os
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º JSON —Ñ–∞–π–ª tracker_db –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–æ–¥–∑–∏
        if os.path.exists('tokens_tracker_database.json'):
            with open('tokens_tracker_database.json', 'r', encoding='utf-8') as f:
                tracker_db = json.load(f)
                
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–æ–¥–∑–∏ –∏–∑ tracker_db
            for token_query, token_data in tracker_db.items():
                if 'emojis' in token_data:
                    tracker_emojis[token_query] = token_data['emojis']
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —ç–º–æ–¥–∑–∏ –∏–∑ tracker_db: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    try:
        for i, token in enumerate(page_tokens, start=start_idx + 1):
            ticker = token.get('ticker', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            query = token.get('query', '')
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—É—é –¥–∞—Ç—É –∏ –≤—Ä–µ–º—è
            if token.get('full_datetime'):
                date_time_str = token.get('full_datetime')
            else:
                added_date = token.get('added_date', '')
                initial_time = token.get('initial_time', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                date_time_str = f"{added_date} {initial_time}" if added_date else initial_time
            
            dexscreener_link = token.get('dexscreener_link', '#')
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–µ–ª
            initial_mc = format_number(token.get('initial_market_cap', 0)) if token.get('initial_market_cap') else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
            current_mc = format_number(token.get('current_market_cap', 0)) if token.get('current_market_cap') else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
            ath_mc = format_number(token.get('ath_market_cap', 0)) if token.get('ath_market_cap') else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç—ã –¥–ª—è ATH –∏ —Ç–µ–∫—É—â–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
            ath_percent = token.get('ath_percent', 0)
            curr_percent = token.get('curr_percent', 0)
            
            ath_percent_str = f"+{ath_percent:.1f}%" if ath_percent >= 0 else f"{ath_percent:.1f}%"
            curr_percent_str = f"+{curr_percent:.1f}%" if curr_percent >= 0 else f"{curr_percent:.1f}%"
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–æ–¥–∑–∏ –¥–ª—è —Ç–æ–∫–µ–Ω–∞ –∏–∑ tracker_db
            emojis = tracker_emojis.get(query, "")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–∫–µ–Ω–µ –≤ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å–æ —Å—Å—ã–ª–∫–æ–π –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ —Ç–∏–∫–µ—Ä–∞
            message += f"{i}. [{ticker}]({dexscreener_link}):\n"
            message += f"   Time: {date_time_str} Mcap: {initial_mc}\n"
            message += f"   {ath_percent_str} ATH {ath_mc}\n"
            message += f"   {curr_percent_str} CURR {current_mc}\n"
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫—É —ç–º–æ–¥–∑–∏ –ø–æ—Å–ª–µ —Å—Ç—Ä–æ–∫–∏ —Å CURR, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
            if emojis:
                message += f"   {emojis}\n"
            
            message += "\n"
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return ("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.", 1, 0)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–º–∞–Ω–¥–∞—Ö
    if page == total_pages - 1:  # –¢–æ–ª—å–∫–æ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        message += f"–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `/clear` –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞–º–∏.\n"
        message += f"–û—Ç–ø—Ä–∞–≤—å—Ç–µ `/excel` –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è Excel —Ñ–∞–π–ª–∞ —Å–æ –≤—Å–µ–º–∏ –¥–∞–Ω–Ω—ã–º–∏."
    
    return (message, total_pages, page)

def format_growth_message(ticker: str, current_multiplier: int, market_cap: str) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –æ —Ä–æ—Å—Ç–µ —Ç–æ–∫–µ–Ω–∞ —Å –æ–≥–æ–Ω—å–∫–∞–º–∏ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –º–Ω–æ–∂–∏—Ç–µ–ª—è."""
    fire_emojis = "üî•" * current_multiplier
    
    return (
        f"{fire_emojis}\n"
        f"–¢–æ–∫–µ–Ω *{ticker}* –≤—ã—Ä–æ—Å –≤ *{current_multiplier}x* –æ—Ç –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è!\n\n"
        f"üí∞ –¢–µ–∫—É—â–∏–π Market Cap: {market_cap}"
    )