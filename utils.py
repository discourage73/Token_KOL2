import datetime
import logging
import time
from typing import Dict, Any, Optional, Union, List

logger = logging.getLogger(__name__)

def format_number(value: Union[int, float, str]) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
    if isinstance(value, (int, float)):
        if value >= 1000000000:
            return f"${value / 1000000000:.2f}B"
        elif value >= 1000000:
            return f"${value / 1000000:.2f}M"
        elif value >= 1000:
            return f"${value / 1000:.2f}K"
        else:
            return f"${value:.2f}"
    elif isinstance(value, str):
        try:
            value = float(value)
            return format_number(value)
        except (ValueError, TypeError):
            return value
    else:
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"

def calculate_token_age(timestamp: Optional[int]) -> str:
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤–æ–∑—Ä–∞—Å—Ç —Ç–æ–∫–µ–Ω–∞ –æ—Ç –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π —Ä–∞–∑–±–∏–≤–∫–æ–π."""
    if not timestamp:
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
    
    try:
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ timestamp –∏–∑ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥ –≤ —Å–µ–∫—É–Ω–¥—ã
        creation_time = datetime.datetime.fromtimestamp(timestamp / 1000)
        now = datetime.datetime.now()
        delta = now - creation_time
        
        days = delta.days
        hours = delta.seconds // 3600
        minutes = (delta.seconds % 3600) // 60
        
        result = []
        
        if days > 0:
            days_str = "–¥–µ–Ω—å" if days == 1 else "–¥–Ω—è" if 1 < days < 5 else "–¥–Ω–µ–π"
            result.append(f"{days} {days_str}")
        
        if hours > 0:
            hours_str = "—á–∞—Å" if hours == 1 else "—á–∞—Å–∞" if 1 < hours < 5 else "—á–∞—Å–æ–≤"
            result.append(f"{hours} {hours_str}")
        
        if minutes > 0 and days == 0:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –º–∏–Ω—É—Ç—ã —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø—Ä–æ—à–ª–æ –º–µ–Ω—å—à–µ –¥–Ω—è
            minutes_str = "–º–∏–Ω—É—Ç–∞" if minutes == 1 else "–º–∏–Ω—É—Ç—ã" if 1 < minutes < 5 else "–º–∏–Ω—É—Ç"
            result.append(f"{minutes} {minutes_str}")
        
        if not result:
            return "–ú–µ–Ω–µ–µ –º–∏–Ω—É—Ç—ã"
        
        return " ".join(result)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ –≤–æ–∑—Ä–∞—Å—Ç–∞ —Ç–æ–∫–µ–Ω–∞: {e}")
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"

def time_elapsed_since(timestamp: float) -> str:
    """–í—ã—á–∏—Å–ª—è–µ—Ç –ø—Ä–æ—à–µ–¥—à–µ–µ –≤—Ä–µ–º—è —Å —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –º–æ–º–µ–Ω—Ç–∞."""
    if not timestamp:
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
    
    try:
        now = time.time()
        delta_seconds = now - timestamp
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ timedelta –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ —Ä–∞—Å—á–µ—Ç–∞
        delta = datetime.timedelta(seconds=delta_seconds)
        
        days = delta.days
        hours = delta.seconds // 3600
        minutes = (delta.seconds % 3600) // 60
        
        result = []
        
        if days > 0:
            days_str = "–¥–µ–Ω—å" if days == 1 else "–¥–Ω—è" if 1 < days < 5 else "–¥–Ω–µ–π"
            result.append(f"{days} {days_str}")
        
        if hours > 0:
            hours_str = "—á–∞—Å" if hours == 1 else "—á–∞—Å–∞" if 1 < hours < 5 else "—á–∞—Å–æ–≤"
            result.append(f"{hours} {hours_str}")
        
        if minutes > 0 and days == 0:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –º–∏–Ω—É—Ç—ã —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø—Ä–æ—à–ª–æ –º–µ–Ω—å—à–µ –¥–Ω—è
            minutes_str = "–º–∏–Ω—É—Ç–∞" if minutes == 1 else "–º–∏–Ω—É—Ç—ã" if 1 < minutes < 5 else "–º–∏–Ω—É—Ç"
            result.append(f"{minutes} {minutes_str}")
        
        if not result:
            return "–ú–µ–Ω–µ–µ –º–∏–Ω—É—Ç—ã"
        
        return " ".join(result)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ –ø—Ä–æ—à–µ–¥—à–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏: {e}")
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"

def process_token_data(token_data: Dict[str, Any]) -> Dict[str, Any]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–∫–µ–Ω–µ."""
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω—É–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    base_token = token_data.get('baseToken', {})
    ticker = base_token.get('symbol', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ').upper()
    ticker_address = base_token.get('address', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
    pair_address = token_data.get('pairAddress', '')
    chain_id = token_data.get('chainId', '')
    
    # –ü–æ–ª—É—á–∞–µ–º market cap, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
    market_cap = token_data.get('fdv')
    raw_market_cap = market_cap  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
    market_cap_formatted = format_number(market_cap)
    
    # –°–æ–∑–¥–∞–µ–º —Å—Å—ã–ª–∫–∏
    dexscreener_link = f"https://dexscreener.com/{chain_id}/{pair_address}"
    axiom_link = f"https://axiom.trade/meme/{pair_address}"
    
    # –ü–æ–ª—É—á–∞–µ–º –æ–±—ä–µ–º —Ç–æ—Ä–≥–æ–≤
    volume_data = token_data.get('volume', {})
    volume_5m = volume_data.get('m5')
    volume_1h = volume_data.get('h1')
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ–±—ä–µ–º—ã
    volume_5m_formatted = format_number(volume_5m)
    volume_1h_formatted = format_number(volume_1h)
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–∞
    pair_created_at = token_data.get('pairCreatedAt')
    token_age = calculate_token_age(pair_created_at)
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç—è—Ö –∏ —Å–∞–π—Ç–∞—Ö
    info = token_data.get('info', {})
    websites = info.get('websites', [])
    socials = info.get('socials', [])
    
    return {
        'ticker': ticker,
        'ticker_address': ticker_address,
        'pair_address': pair_address,
        'chain_id': chain_id,
        'market_cap': market_cap_formatted,
        'raw_market_cap': raw_market_cap,
        'volume_5m': volume_5m_formatted,
        'volume_1h': volume_1h_formatted,
        'token_age': token_age,
        'dexscreener_link': dexscreener_link,
        'axiom_link': axiom_link,
        'websites': websites,
        'socials': socials
    }

def format_message(token_info: Dict[str, Any], initial_data: Optional[Dict[str, Any]] = None) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ç–æ–∫–µ–Ω–µ."""
    message = f"ü™ô *–¢–∏–∫–µ—Ä*: {token_info['ticker']}\n"
    message += f"üìù *–ê–¥—Ä–µ—Å*: `{token_info['ticker_address']}`\n\n"
    
    message += f"üí∞ *Market Cap*: {token_info['market_cap']}\n"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ –º–∞—Ä–∫–µ—Ç –∫–∞–ø–∞, –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–∞—á–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    if (initial_data and 'raw_market_cap' in initial_data and 
        initial_data['raw_market_cap'] and token_info['raw_market_cap']):
        initial_mcap = initial_data['raw_market_cap']
        current_mcap = token_info['raw_market_cap']
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–Ω–æ–∂–∏—Ç–µ–ª—å
        if initial_mcap > 0:
            multiplier = current_mcap / initial_mcap
            
            # –ï—Å–ª–∏ –º–Ω–æ–∂–∏—Ç–µ–ª—å –±–æ–ª—å—à–µ 2, –¥–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            if multiplier >= 2:
                # –û–∫—Ä—É–≥–ª—è–µ–º –º–Ω–æ–∂–∏—Ç–µ–ª—å –¥–æ —Ü–µ–ª–æ–≥–æ —á–∏—Å–ª–∞, –µ—Å–ª–∏ –æ–Ω –±–æ–ª—å—à–µ 10, –∏–Ω–∞—á–µ –¥–æ –æ–¥–Ω–æ–≥–æ –∑–Ω–∞–∫–∞ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π
                mult_formatted = int(multiplier) if multiplier >= 10 else round(multiplier, 1)
                message += f"üî• *–†–æ—Å—Ç: x{mult_formatted}* –æ—Ç –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è!\n"
    
    message += f"‚è± *–í–æ–∑—Ä–∞—Å—Ç —Ç–æ–∫–µ–Ω–∞*: {token_info['token_age']}\n\n"
    
    # –ë–ª–æ–∫ —Å –æ–±—ä–µ–º–∞–º–∏ —Ç–æ—Ä–≥–æ–≤
    volumes_block = ""
    if token_info['volume_5m'] != "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ":
        volumes_block += f"üìà *–û–±—ä–µ–º (5–º)*: {token_info['volume_5m']}\n"
        
    if token_info['volume_1h'] != "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ":
        volumes_block += f"üìà *–û–±—ä–µ–º (1—á)*: {token_info['volume_1h']}\n"
    
    if volumes_block:
        message += volumes_block + "\n"
    
    # –ë–ª–æ–∫ —Å —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ —Ç–æ—Ä–≥–æ–≤—ã–µ –ø–ª–æ—â–∞–¥–∫–∏
    message += f"üîé *–°—Å—ã–ª–∫–∏*: [DexScreener]({token_info['dexscreener_link']}) | [Axiom Trade]({token_info['axiom_link']})\n\n"
    
    # –ë–ª–æ–∫ —Å —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ —Å–∞–π—Ç—ã
    if token_info['websites']:
        website_links = [f"[{website.get('label', 'Website')}]({website.get('url', '')})" 
                         for website in token_info['websites'] if website.get('url')]
        
        if website_links:
            message += f"üåê *–°–∞–π—Ç—ã*: {' | '.join(website_links)}\n"
    
    # –ë–ª–æ–∫ —Å —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ —Å–æ—Ü—Å–µ—Ç–∏
    if token_info['socials']:
        social_links = [f"[{social.get('type', '').capitalize()}]({social.get('url', '')})" 
                        for social in token_info['socials'] if social.get('url') and social.get('type')]
        
        if social_links:
            message += f"üì± *–°–æ—Ü—Å–µ—Ç–∏*: {' | '.join(social_links)}\n"
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫—É –≤—Ä–µ–º–µ–Ω–∏ –∏ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    current_time = datetime.datetime.now().strftime("%H:%M:%S")
    message += f"\n_–û–±–Ω–æ–≤–ª–µ–Ω–æ: {current_time}_"
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ, –µ—Å–ª–∏ –æ–Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–∞
    if initial_data:
        message += f"\n_–ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å: {initial_data['time']}_"
        message += f"\n_–ù–∞—á–∞–ª—å–Ω—ã–π Market Cap: {initial_data['market_cap']}_"
    
    return message

def format_tokens_list(tokens_data: Dict[str, Dict[str, Any]]) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
    if not tokens_data:
        return "–ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞."
    
    message = f"üìã *–°–ø–∏—Å–æ–∫ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ ({len(tokens_data)} —à—Ç.)*\n\n"
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω–æ–≤
    growing_tokens = []
    falling_tokens = []
    
    for query, data in tokens_data.items():
        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π –º–∞—Ä–∫–µ—Ç –∫–∞–ø
        initial_market_cap = 0
        if 'initial_data' in data and 'raw_market_cap' in data['initial_data']:
            initial_market_cap = data['initial_data'].get('raw_market_cap', 0)
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π –º–∞—Ä–∫–µ—Ç –∫–∞–ø –∏–∑ token_info, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
        current_market_cap = 0
        if 'token_info' in data and 'raw_market_cap' in data['token_info']:
            current_market_cap = data['token_info'].get('raw_market_cap', 0)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º ATH –º–∞—Ä–∫–µ—Ç –∫–∞–ø –∏–∑ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        ath_market_cap = data.get('ath_market_cap', 0)
        
        # –ï—Å–ª–∏ ATH –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–ª–∏ –º–µ–Ω—å—à–µ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π –∫–∞–∫ ATH
        if not ath_market_cap or (initial_market_cap > ath_market_cap):
            ath_market_cap = initial_market_cap
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –º–Ω–æ–∂–∏—Ç–µ–ª—å —Ä–æ—Å—Ç–∞ –æ—Ç –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –¥–æ ATH
        max_multiplier = 1
        if initial_market_cap and ath_market_cap and initial_market_cap > 0:
            max_multiplier = ath_market_cap / initial_market_cap
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –º–∞—Ä–∫–µ—Ç –∫–∞–ø–∞
        current_multiplier = 1
        if initial_market_cap and current_market_cap and initial_market_cap > 0:
            current_multiplier = current_market_cap / initial_market_cap
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–∏–∫–µ—Ä –∏–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–æ–∫–µ–Ω–µ
        ticker = query
        if 'token_info' in data and 'ticker' in data['token_info']:
            ticker = data['token_info'].get('ticker', query)
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ DexScreener
        dexscreener_link = "#"  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if 'token_info' in data and 'dexscreener_link' in data['token_info']:
            dexscreener_link = data['token_info'].get('dexscreener_link', "#")
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–æ–∫–µ–Ω–µ
        token_info = {
            'query': query,
            'ticker': ticker,
            'initial_market_cap': initial_market_cap,
            'market_cap': current_market_cap,
            'ath_market_cap': ath_market_cap,
            'max_multiplier': max_multiplier,
            'current_multiplier': current_multiplier,
            'dexscreener_link': dexscreener_link,
            # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç –ø–∞–¥–µ–Ω–∏—è –æ—Ç –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è, –µ—Å–ª–∏ –µ—Å—Ç—å
            'decline_percent': 0 if current_multiplier >= 1 else (1 - current_multiplier) * 100
        }
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–æ–∫–µ–Ω—ã –Ω–∞ —Ä–∞—Å—Ç—É—â–∏–µ –∏ –ø–∞–¥–∞—é—â–∏–µ
        if current_multiplier >= 1:
            growing_tokens.append(token_info)
        else:
            falling_tokens.append(token_info)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–∞—Å—Ç—É—â–∏–µ —Ç–æ–∫–µ–Ω—ã –ø–æ —É–±—ã–≤–∞–Ω–∏—é –º–Ω–æ–∂–∏—Ç–µ–ª—è
    growing_tokens.sort(key=lambda x: x['max_multiplier'], reverse=True)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–∞–¥–∞—é—â–∏–µ —Ç–æ–∫–µ–Ω—ã –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é –ø—Ä–æ—Ü–µ–Ω—Ç–∞ –ø–∞–¥–µ–Ω–∏—è (–æ—Ç –º–µ–Ω—å—à–µ–π –ø–æ—Ç–µ—Ä–∏ –∫ –±–æ–ª—å—à–µ–π)
    falling_tokens.sort(key=lambda x: x['decline_percent'])
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–∞—Å—Ç—É—â–∏–µ —Ç–æ–∫–µ–Ω—ã
    for i, token in enumerate(growing_tokens, 1):
        ticker = token['ticker']
        current_mc = format_number(token['market_cap']) if token['market_cap'] else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
        ath_mc = format_number(token['ath_market_cap']) if token['ath_market_cap'] else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
        dexscreener_link = token['dexscreener_link']
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –º–Ω–æ–∂–∏—Ç–µ–ª—å —Ä–æ—Å—Ç–∞
        max_mult = token['max_multiplier']
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –º–Ω–æ–∂–∏—Ç–µ–ª—å
        if max_mult >= 2:
            # –ï—Å–ª–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –º–Ω–æ–∂–∏—Ç–µ–ª—å –±–æ–ª—å—à–µ –∏–ª–∏ —Ä–∞–≤–µ–Ω 2, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞–∫ "xN"
            growth_mult_str = f"x{int(max_mult)}" if max_mult >= 10 else f"x{max_mult:.1f}"
        else:
            # –ï—Å–ª–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –º–Ω–æ–∂–∏—Ç–µ–ª—å –º–µ–Ω—å—à–µ 2, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞–∫ "+N%"
            growth_percent = (max_mult - 1) * 100
            growth_mult_str = f"+{growth_percent:.1f}%"
        
        # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ç–æ–∫–µ–Ω–µ –≤ –ø–æ—Ä—è–¥–∫–µ: –º–Ω–æ–∂–∏—Ç–µ–ª—å | ATH | CURR
        token_line = f"{i}. [*{ticker}*]({dexscreener_link}): {growth_mult_str}"
        
        # –î–æ–±–∞–≤–ª—è–µ–º ATH –º–∞—Ä–∫–µ—Ç –∫–∞–ø
        token_line += f" | ATH {ath_mc}"
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –º–∞—Ä–∫–µ—Ç –∫–∞–ø
        token_line += f" | CURR {current_mc}"
        
        message += token_line + "\n"
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å, –µ—Å–ª–∏ –µ—Å—Ç—å –∏ —Ä–∞—Å—Ç—É—â–∏–µ, –∏ –ø–∞–¥–∞—é—â–∏–µ —Ç–æ–∫–µ–Ω—ã
    if growing_tokens and falling_tokens:
        message += "\n"  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –∫–∞–∫ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø–∞–¥–∞—é—â–∏–µ —Ç–æ–∫–µ–Ω—ã
    for i, token in enumerate(falling_tokens, len(growing_tokens) + 1):
        ticker = token['ticker']
        current_mc = format_number(token['market_cap']) if token['market_cap'] else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
        ath_mc = format_number(token['ath_market_cap']) if token['ath_market_cap'] else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
        dexscreener_link = token['dexscreener_link']
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç –ø–∞–¥–µ–Ω–∏—è
        decline_percent = token['decline_percent']
        decline_str = f"-{decline_percent:.1f}%"
        
        # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ç–æ–∫–µ–Ω–µ –≤ –ø–æ—Ä—è–¥–∫–µ: –ø–∞–¥–µ–Ω–∏–µ | ATH | CURR
        token_line = f"{i}. [*{ticker}*]({dexscreener_link}): {decline_str}"
        
        # –î–æ–±–∞–≤–ª—è–µ–º ATH –º–∞—Ä–∫–µ—Ç –∫–∞–ø
        token_line += f" | ATH {ath_mc}"
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –º–∞—Ä–∫–µ—Ç –∫–∞–ø
        token_line += f" | CURR {current_mc}"
        
        message += token_line + "\n"
    
        message += "\n_–û—Ç–ø—Ä–∞–≤—å—Ç–µ_ `/delete –¢–ò–ö–ï–†` _–¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞ –∏–∑ —Å–ø–∏—Å–∫–∞._"
    
    return message

def remove_specific_token(token_storage, token_query: str):
    """
    –£–¥–∞–ª—è–µ—Ç —É–∫–∞–∑–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω –∏–∑ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞.
    
    Args:
        token_storage: –ú–æ–¥—É–ª—å —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤
        token_query: –ó–∞–ø—Ä–æ—Å –∏–ª–∏ —Ç–∏–∫–µ—Ä —Ç–æ–∫–µ–Ω–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
    
    Returns:
        dict: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± —É–¥–∞–ª–µ–Ω–Ω–æ–º —Ç–æ–∫–µ–Ω–µ –∏–ª–∏ None, –µ—Å–ª–∏ —Ç–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω
    """
    # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º –≤—Å–µ —Ç–æ–∫–µ–Ω—ã
    all_tokens = token_storage.get_all_tokens()
    
    # –ò—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –∑–∞–ø—Ä–æ—Å—É/–∞–¥—Ä–µ—Å—É
    if token_query in all_tokens:
        token_data = all_tokens[token_query]
        token_storage.remove_token_data(token_query)
        return {"query": token_query, "data": token_data}
    
    # –ï—Å–ª–∏ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–µ—Ç, –∏—â–µ–º –ø–æ —Ç–∏–∫–µ—Ä—É (–Ω–µ—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∫ —Ä–µ–≥–∏—Å—Ç—Ä—É)
    for query, data in all_tokens.items():
        ticker = ""
        if 'token_info' in data and 'ticker' in data['token_info']:
            ticker = data['token_info']['ticker']
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–∏–∫–µ—Ä–∞ (–Ω–µ—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∫ —Ä–µ–≥–∏—Å—Ç—Ä—É)
        if ticker.lower() == token_query.lower():
            token_data = all_tokens[query]
            token_storage.remove_token_data(query)
            return {"query": query, "data": token_data}
    
    # –ï—Å–ª–∏ —Ç–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω
    return None